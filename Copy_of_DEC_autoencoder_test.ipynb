{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "svO8F_9MRerU",
    "outputId": "062936ac-3516-4022-eaf9-31736ab741a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dfg69_VfbXuf",
    "outputId": "77df6ec1-1309-441e-f4da-0663b1e04d2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RDPZZ3uQRtsO"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOcE_HzaR0MF"
   },
   "outputs": [],
   "source": [
    "original_dim = 28*28\n",
    "hidden_dim_1 = 500\n",
    "hidden_dim_2 = 500\n",
    "hidden_dim_3 = 2000\n",
    "encoded_dim = 10\n",
    "dimentions = [original_dim, hidden_dim_1, hidden_dim_2, hidden_dim_3, encoded_dim]\n",
    "n_layers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PnPNtJAsd2Tn"
   },
   "outputs": [],
   "source": [
    "def get_layer_autoencoder(input_dim, output_dim):\n",
    "  \"\"\"\n",
    "  This function generates autoencoder layer \n",
    "  with specified input and output dimentions\n",
    "  return:3 models:  autoencoder, encoder and decoder\n",
    "  \"\"\"\n",
    "  input_ = Input(shape=(input_dim,))\n",
    "  encoder_dropout = Dropout(0.2)(input_)\n",
    "  if output_dim == encoded_dim:\n",
    "    encoder_dense = Dense(output_dim, activation='linear')(encoder_dropout)\n",
    "  else:\n",
    "    encoder_dense = Dense(output_dim, activation='relu')(encoder_dropout)\n",
    "  encoded_input = Input(shape=(output_dim,))\n",
    "\n",
    "  decoder_dropout = Dropout(0.2)(encoder_dense)\n",
    "  if output_dim == original_dim:\n",
    "    decoder_dense = Dense(input_dim, activation='linear')(decoder_dropout)\n",
    "  else:\n",
    "    decoder_dense = Dense(input_dim, activation='relu')(decoder_dropout)\n",
    "\n",
    "  autoencoder = Model(input_, decoder_dense)\n",
    " \n",
    "  decoder_dense_layer = autoencoder.layers[-1]\n",
    "  decoder_dropout_layer = autoencoder.layers[-2]\n",
    "\n",
    "\n",
    "  #create encoder and decoder\n",
    "  encoder = Model(input_, encoder_dense)\n",
    "  decoder = Model(encoded_input, decoder_dense_layer(decoder_dropout_layer\n",
    "                                                     (encoded_input)))\n",
    "  autoencoder.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return autoencoder, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pjp94j4nB6-S"
   },
   "outputs": [],
   "source": [
    "def train_autoencoder_layers_separately():\n",
    "  autoencoders = []\n",
    "  encoders = []\n",
    "  decoders = []\n",
    "  inputs_test = [x_test]\n",
    "  inputs_train = [x_train]\n",
    "  input_i_test = x_test\n",
    "  input_i_train = x_train\n",
    "  for i in range(n_layers):\n",
    "    autoencoder, encoder, decoder = get_layer_autoencoder(dimentions[i], dimentions[i+1])\n",
    "    print(f'<========================================{i + 1} autoencoder layer generated========================================>')\n",
    "\n",
    "    autoencoder.fit(inputs_train[i], inputs_train[i],\n",
    "                     epochs=50,\n",
    "                     batch_size=256,\n",
    "                     validation_data=(inputs_test[i], inputs_test[i]))\n",
    "    print(f'<========================================{i + 1} autoencoder layer trained========================================>')\n",
    "    autoencoders.append(autoencoder)\n",
    "    encoders.append(encoder)\n",
    "    decoders.append(decoder)\n",
    "    inputs_train.append(encoder.predict(inputs_train[i]))\n",
    "    inputs_test.append(encoder.predict(inputs_test[i]))\n",
    "  return autoencoders, encoders, decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7194
    },
    "colab_type": "code",
    "id": "LKHXZW12B62P",
    "outputId": "1827d8d2-d9d9-4017-e3fd-d07bafa4d393"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<========================================1 autoencoder layer generated========================================>\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 0.0274 - val_loss: 0.0105\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0150 - val_loss: 0.0087\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 0.0132 - val_loss: 0.0076\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0120 - val_loss: 0.0069\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 0.0111 - val_loss: 0.0064\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.0105 - val_loss: 0.0061\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.0100 - val_loss: 0.0058\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 0.0097 - val_loss: 0.0056\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.0093 - val_loss: 0.0054\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.0091 - val_loss: 0.0052\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.0089 - val_loss: 0.0051\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.0087 - val_loss: 0.0052\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.0086 - val_loss: 0.0049\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.0085 - val_loss: 0.0049\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 0.0084 - val_loss: 0.0049\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0083 - val_loss: 0.0049\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0082 - val_loss: 0.0048\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0082 - val_loss: 0.0047\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0081 - val_loss: 0.0047\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.0081 - val_loss: 0.0047\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.0080 - val_loss: 0.0049\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.0080 - val_loss: 0.0047\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 0.0079 - val_loss: 0.0047\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 0.0079 - val_loss: 0.0046\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 0.0079 - val_loss: 0.0046\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 0.0078 - val_loss: 0.0046\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.0078 - val_loss: 0.0046\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0078 - val_loss: 0.0046\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.0078 - val_loss: 0.0046\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.0077 - val_loss: 0.0046\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.0077 - val_loss: 0.0046\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 0.0077 - val_loss: 0.0045\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.0077 - val_loss: 0.0045\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.0076 - val_loss: 0.0045\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.0076 - val_loss: 0.0045\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.0076 - val_loss: 0.0046\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0076 - val_loss: 0.0045\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0076 - val_loss: 0.0045\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.0076 - val_loss: 0.0045\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.0075 - val_loss: 0.0045\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.0075 - val_loss: 0.0045\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 0.0075 - val_loss: 0.0045\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.0075 - val_loss: 0.0044\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.0075 - val_loss: 0.0044\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 0.0075 - val_loss: 0.0044\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0075 - val_loss: 0.0044\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.0075 - val_loss: 0.0044\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.0074 - val_loss: 0.0044\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0074 - val_loss: 0.0044\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.0074 - val_loss: 0.0044\n",
      "<========================================1 autoencoder layer trained========================================>\n",
      "<========================================2 autoencoder layer generated========================================>\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.0289 - val_loss: 0.0107\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.0148 - val_loss: 0.0077\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.0123 - val_loss: 0.0064\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.0107 - val_loss: 0.0055\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0096 - val_loss: 0.0047\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.0087 - val_loss: 0.0041\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.0081 - val_loss: 0.0037\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0077 - val_loss: 0.0036\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.0074 - val_loss: 0.0034\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.0072 - val_loss: 0.0032\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.0070 - val_loss: 0.0030\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.0068 - val_loss: 0.0030\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.0067 - val_loss: 0.0029\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0066 - val_loss: 0.0029\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0065 - val_loss: 0.0027\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.0065 - val_loss: 0.0027\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.0064 - val_loss: 0.0027\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.0063 - val_loss: 0.0027\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.0063 - val_loss: 0.0027\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.0063 - val_loss: 0.0026\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.0062 - val_loss: 0.0026\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.0062 - val_loss: 0.0026\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.0062 - val_loss: 0.0026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.0061 - val_loss: 0.0026\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.0061 - val_loss: 0.0026\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.0061 - val_loss: 0.0026\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.0061 - val_loss: 0.0025\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.0060 - val_loss: 0.0026\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.0060 - val_loss: 0.0025\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.0060 - val_loss: 0.0025\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.0060 - val_loss: 0.0025\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.0060 - val_loss: 0.0025\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.0060 - val_loss: 0.0025\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.0059 - val_loss: 0.0025\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.0059 - val_loss: 0.0025\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.0059 - val_loss: 0.0025\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.0059 - val_loss: 0.0024\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.0059 - val_loss: 0.0025\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.0059 - val_loss: 0.0025\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.0059 - val_loss: 0.0024\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.0059 - val_loss: 0.0025\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0059 - val_loss: 0.0025\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.0058 - val_loss: 0.0024\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.0058 - val_loss: 0.0024\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.0058 - val_loss: 0.0024\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.0058 - val_loss: 0.0024\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.0058 - val_loss: 0.0024\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 0.0058 - val_loss: 0.0024\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.0058 - val_loss: 0.0024\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.0058 - val_loss: 0.0024\n",
      "<========================================2 autoencoder layer trained========================================>\n",
      "<========================================3 autoencoder layer generated========================================>\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 14s 229us/step - loss: 0.0121 - val_loss: 0.0047\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 13s 219us/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 14s 230us/step - loss: 0.0058 - val_loss: 0.0031\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 13s 218us/step - loss: 0.0050 - val_loss: 0.0026\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 13s 215us/step - loss: 0.0046 - val_loss: 0.0024\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 13s 212us/step - loss: 0.0043 - val_loss: 0.0023\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 13s 213us/step - loss: 0.0041 - val_loss: 0.0023\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 13s 212us/step - loss: 0.0039 - val_loss: 0.0020\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 13s 211us/step - loss: 0.0038 - val_loss: 0.0020\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 13s 211us/step - loss: 0.0037 - val_loss: 0.0020\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 13s 213us/step - loss: 0.0036 - val_loss: 0.0019\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 13s 215us/step - loss: 0.0035 - val_loss: 0.0019\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 13s 212us/step - loss: 0.0035 - val_loss: 0.0018\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 13s 214us/step - loss: 0.0034 - val_loss: 0.0019\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 13s 217us/step - loss: 0.0034 - val_loss: 0.0018\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.0034 - val_loss: 0.0018\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 13s 215us/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 13s 211us/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 13s 212us/step - loss: 0.0033 - val_loss: 0.0017\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 13s 216us/step - loss: 0.0033 - val_loss: 0.0017\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 0.0032 - val_loss: 0.0017\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 13s 211us/step - loss: 0.0032 - val_loss: 0.0018\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 0.0032 - val_loss: 0.0017\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 0.0032 - val_loss: 0.0018\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 13s 211us/step - loss: 0.0032 - val_loss: 0.0017\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 0.0032 - val_loss: 0.0017\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 15s 247us/step - loss: 0.0032 - val_loss: 0.0018\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 13s 218us/step - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 13s 217us/step - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 13s 209us/step - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 13s 214us/step - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 13s 217us/step - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 13s 211us/step - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 13s 211us/step - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 0.0030 - val_loss: 0.0017\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.0030 - val_loss: 0.0017\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 13s 213us/step - loss: 0.0030 - val_loss: 0.0017\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 0.0030 - val_loss: 0.0017\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 13s 215us/step - loss: 0.0030 - val_loss: 0.0017\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 13s 213us/step - loss: 0.0030 - val_loss: 0.0017\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 13s 214us/step - loss: 0.0030 - val_loss: 0.0017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 13s 217us/step - loss: 0.0030 - val_loss: 0.0017\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 13s 213us/step - loss: 0.0030 - val_loss: 0.0017\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 13s 219us/step - loss: 0.0030 - val_loss: 0.0017\n",
      "<========================================3 autoencoder layer trained========================================>\n",
      "<========================================4 autoencoder layer generated========================================>\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 8s 125us/step - loss: 0.0053 - val_loss: 0.0042\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.004 - 5s 90us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.0046 - val_loss: 0.0042\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.0046 - val_loss: 0.0042\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0046 - val_loss: 0.0042\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "<========================================4 autoencoder layer trained========================================>\n"
     ]
    }
   ],
   "source": [
    "autoencoders, encoders, decoders = train_autoencoder_layers_separately()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MkTiORNVL-cv",
    "outputId": "7ff80f09-7f41-42ce-9900-066f466b085d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x7fd60f6454a8>"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoders[3].layers[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "XdkAXqBXHOus",
    "outputId": "0068a4ca-70b0-43ce-ab80-7d5ee41e4ae3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,)\n",
      "(500,)\n",
      "(2000,)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# fine tune full autoencoder\n",
    "\n",
    "# autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "from keras.models import Sequential\n",
    "input_ = Input(shape=(original_dim,))\n",
    "encode1 = autoencoders[0].layers[1](input_)\n",
    "# print(encode1.get_input_shape_at(0))\n",
    "encode1 = autoencoders[0].layers[2](encode1)\n",
    "print(encode1[0].shape)\n",
    "encode2 = autoencoders[1].layers[1](encode1)\n",
    "encode2 = autoencoders[1].layers[2](encode2)\n",
    "print(encode2[0].shape)\n",
    "encode3 = autoencoders[2].layers[1](encode2)\n",
    "encode3 = autoencoders[2].layers[2](encode3)\n",
    "print(encode3[0].shape)\n",
    "encode4 = autoencoders[3].layers[1](encode3)\n",
    "encode4 = autoencoders[3].layers[2](encode4)\n",
    "print(encode4[0].shape)\n",
    "\n",
    "decode4 = autoencoders[3].layers[3](encode4)\n",
    "decode4 = autoencoders[3].layers[4](decode4)\n",
    "decode3 = autoencoders[2].layers[3](decode4)\n",
    "decode3 = autoencoders[2].layers[4](decode3)\n",
    "decode2 = autoencoders[1].layers[3](decode3)\n",
    "decode2 = autoencoders[1].layers[4](decode2)\n",
    "decode1 = autoencoders[0].layers[3](decode2)\n",
    "decode1 = autoencoders[0].layers[4](decode1)\n",
    "\n",
    "\n",
    "sae_autoencoder = Model(inputs=input_, outputs=decode1)\n",
    "\n",
    "# encode_layers = []\n",
    "# decode_layers = []\n",
    "# for i in range(n_layers):\n",
    "#   encode_layers.extend([autoencoders[i].layers[0], autoencoders[i].layers[1]])\n",
    "#   decode_layers.insert(0, autoencoders[i].layers[-1])  # decode layers are in reverse order\n",
    "#   decode_layers.insert(0, autoencoders[i].layers[-2])\n",
    "\n",
    "# layers = encode_layers + decode_layers\n",
    "# sae_autoencoder = Model(layers)\n",
    "sae_autoencoder.compile(loss='mean_squared_error', optimizer='adam',  metrics=['accuracy'])\n",
    "# print(sae_autoencoder.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-80bea5481fc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "!pip install goo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-35854038ca30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msae_autoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sae_autoencoder.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sae_autoencoder.save('sae_autoencoder.h5')\n",
    "from google.colab import files\n",
    "files.download( \"sae_autoencoder.h5\" ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "CSBnkrFGLYXR",
    "outputId": "fccc4934-8465-47ef-a1b5-a40a2c9e5bee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Network.summary of <keras.engine.training.Model object at 0xb9a047c18>>\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 44s 735us/step - loss: 0.0381 - acc: 0.0101 - val_loss: 0.0253 - val_acc: 0.0140\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 36s 593us/step - loss: 0.0341 - acc: 0.0113 - val_loss: 0.0242 - val_acc: 0.0133\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 35s 584us/step - loss: 0.0326 - acc: 0.0099 - val_loss: 0.0233 - val_acc: 0.0089\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 32s 530us/step - loss: 0.0318 - acc: 0.0107 - val_loss: 0.0227 - val_acc: 0.0121\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 30s 501us/step - loss: 0.0313 - acc: 0.0106 - val_loss: 0.0225 - val_acc: 0.0150\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 30s 496us/step - loss: 0.0309 - acc: 0.0102 - val_loss: 0.0225 - val_acc: 0.0110\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 29s 491us/step - loss: 0.0307 - acc: 0.0107 - val_loss: 0.0220 - val_acc: 0.0114\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 28s 474us/step - loss: 0.0303 - acc: 0.0103 - val_loss: 0.0223 - val_acc: 0.0096\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 28s 471us/step - loss: 0.0301 - acc: 0.0108 - val_loss: 0.0223 - val_acc: 0.0127\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 28s 464us/step - loss: 0.0299 - acc: 0.0109 - val_loss: 0.0221 - val_acc: 0.0111\n"
     ]
    }
   ],
   "source": [
    "print(sae_autoencoder.summary)\n",
    "history = sae_autoencoder.fit(x_train, x_train,\n",
    "                     epochs=10,\n",
    "                     batch_size=256,\n",
    "                     validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "id": "8zieeY1EPijY",
    "outputId": "b45c779b-2484-4cb9-bac2-f5c0c7ec1b88"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "2Qjmh6HqvlcN",
    "outputId": "f91a382d-6bd9-47d5-cd92-82f2d46b3049"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sae_autoencoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-68ba98ece6e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msae_autoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sae_autoencoder' is not defined"
     ]
    }
   ],
   "source": [
    "sae_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Sk_tYc1vlPy"
   },
   "outputs": [],
   "source": [
    "predicted = sae_autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dMDK8biLwfYH"
   },
   "outputs": [],
   "source": [
    "for i in predicted:\n",
    "  i.reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sX93wUfVwpfw",
    "outputId": "ed328c7f-8191-4f92-b167-109c1b1d3a90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "ZpgxkJnhxuSj",
    "outputId": "5d6d38c8-7653-4d78-91fd-375b9f62abb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TU_EPPrXx6Ou",
    "outputId": "ed8a54ef-43fc-485b-895f-99202ffce1dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "3ZBdcGxZyjvk",
    "outputId": "a9bc94cc-0741-48ca-fc1f-2e2bda1239ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive  gdrive  sae_autoencoder.h5  sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nkswlpcyyp6w"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download( \"sae_autoencoder.h5\" ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q5GAcLZxyMeg"
   },
   "outputs": [],
   "source": [
    "sae_autoencoder.save('sae_autoencoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "colab_type": "code",
    "id": "mXOprDsgdgeJ",
    "outputId": "6882d866-7479-4d19-e8b7-bd4281d240f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "(784,)\n",
      "(784,)\n",
      "(784,)\n",
      "(784,)\n",
      "(784,)\n",
      "(784,)\n",
      "(784,)\n",
      "(784,)\n",
      "(784,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABGoAAADjCAYAAADdR/IFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# sae_autoencoder.predict(x_test[i])\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    print(x_test[i].shape)\n",
    "    plt.imshow(predicted[i].reshape(28,28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DMXe-yWodkQQ"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "sae_autoencoder = load_model('sae_autoencoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_autoencoder.compile(loss='mean_squared_error', optimizer='adam',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 30s 506us/step - loss: 0.0382 - acc: 0.0101 - val_loss: 0.0257 - val_acc: 0.0102\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 32s 541us/step - loss: 0.0339 - acc: 0.0094 - val_loss: 0.0241 - val_acc: 0.0122\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 30s 506us/step - loss: 0.0327 - acc: 0.0094 - val_loss: 0.0234 - val_acc: 0.0107\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 28s 471us/step - loss: 0.0319 - acc: 0.0103 - val_loss: 0.0230 - val_acc: 0.0125\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 28s 474us/step - loss: 0.0313 - acc: 0.0100 - val_loss: 0.0228 - val_acc: 0.0099\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 29s 482us/step - loss: 0.0308 - acc: 0.0097 - val_loss: 0.0222 - val_acc: 0.0092\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 30s 508us/step - loss: 0.0306 - acc: 0.0096 - val_loss: 0.0225 - val_acc: 0.0130\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 31s 517us/step - loss: 0.0303 - acc: 0.0095 - val_loss: 0.0218 - val_acc: 0.0137\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 31s 522us/step - loss: 0.0301 - acc: 0.0105 - val_loss: 0.0224 - val_acc: 0.0114\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 33s 545us/step - loss: 0.0299 - acc: 0.0100 - val_loss: 0.0217 - val_acc: 0.0122\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 29s 475us/step - loss: 0.0298 - acc: 0.0099 - val_loss: 0.0224 - val_acc: 0.0084\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 0.0296 - acc: 0.0110 - val_loss: 0.0219 - val_acc: 0.0110\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 28s 465us/step - loss: 0.0295 - acc: 0.0106 - val_loss: 0.0216 - val_acc: 0.0112\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 29s 480us/step - loss: 0.0294 - acc: 0.0099 - val_loss: 0.0216 - val_acc: 0.0121\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 29s 487us/step - loss: 0.0294 - acc: 0.0102 - val_loss: 0.0220 - val_acc: 0.0126\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 28s 467us/step - loss: 0.0291 - acc: 0.0107 - val_loss: 0.0221 - val_acc: 0.0124\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 29s 490us/step - loss: 0.0292 - acc: 0.0103 - val_loss: 0.0214 - val_acc: 0.0124\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 27s 448us/step - loss: 0.0290 - acc: 0.0108 - val_loss: 0.0215 - val_acc: 0.0133\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 28s 474us/step - loss: 0.0290 - acc: 0.0103 - val_loss: 0.0215 - val_acc: 0.0154\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 28s 473us/step - loss: 0.0288 - acc: 0.0101 - val_loss: 0.0217 - val_acc: 0.0106\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 30s 492us/step - loss: 0.0288 - acc: 0.0099 - val_loss: 0.0215 - val_acc: 0.0140\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 30s 492us/step - loss: 0.0287 - acc: 0.0108 - val_loss: 0.0214 - val_acc: 0.0135\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 28s 469us/step - loss: 0.0289 - acc: 0.0107 - val_loss: 0.0217 - val_acc: 0.0147\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 33s 544us/step - loss: 0.0287 - acc: 0.0105 - val_loss: 0.0212 - val_acc: 0.0126\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 32s 525us/step - loss: 0.0287 - acc: 0.0101 - val_loss: 0.0213 - val_acc: 0.0115\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 31s 511us/step - loss: 0.0286 - acc: 0.0101 - val_loss: 0.0214 - val_acc: 0.0128\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 30s 508us/step - loss: 0.0285 - acc: 0.0103 - val_loss: 0.0215 - val_acc: 0.0146\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 28s 465us/step - loss: 0.0285 - acc: 0.0110 - val_loss: 0.0213 - val_acc: 0.0120\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 29s 486us/step - loss: 0.0284 - acc: 0.0106 - val_loss: 0.0218 - val_acc: 0.0128\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 32s 528us/step - loss: 0.0284 - acc: 0.0104 - val_loss: 0.0211 - val_acc: 0.0119\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 30s 496us/step - loss: 0.0283 - acc: 0.0100 - val_loss: 0.0213 - val_acc: 0.0155\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 29s 484us/step - loss: 0.0283 - acc: 0.0099 - val_loss: 0.0216 - val_acc: 0.0139\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 29s 487us/step - loss: 0.0283 - acc: 0.0105 - val_loss: 0.0208 - val_acc: 0.0130\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 29s 490us/step - loss: 0.0282 - acc: 0.0113 - val_loss: 0.0214 - val_acc: 0.0126\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 32s 529us/step - loss: 0.0282 - acc: 0.0111 - val_loss: 0.0215 - val_acc: 0.0140\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 29s 489us/step - loss: 0.0282 - acc: 0.0112 - val_loss: 0.0213 - val_acc: 0.0120\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 30s 500us/step - loss: 0.0282 - acc: 0.0110 - val_loss: 0.0212 - val_acc: 0.0114\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 29s 490us/step - loss: 0.0282 - acc: 0.0102 - val_loss: 0.0214 - val_acc: 0.0153\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 27s 451us/step - loss: 0.0282 - acc: 0.0110 - val_loss: 0.0214 - val_acc: 0.0112\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 27s 454us/step - loss: 0.0281 - acc: 0.0106 - val_loss: 0.0212 - val_acc: 0.0130\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 30s 508us/step - loss: 0.0281 - acc: 0.0106 - val_loss: 0.0211 - val_acc: 0.0130\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 29s 481us/step - loss: 0.0280 - acc: 0.0106 - val_loss: 0.0215 - val_acc: 0.0117\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 31s 511us/step - loss: 0.0280 - acc: 0.0113 - val_loss: 0.0209 - val_acc: 0.0143\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 31s 509us/step - loss: 0.0280 - acc: 0.0098 - val_loss: 0.0211 - val_acc: 0.0138\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 28s 467us/step - loss: 0.0280 - acc: 0.0101 - val_loss: 0.0212 - val_acc: 0.0120\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 27s 454us/step - loss: 0.0279 - acc: 0.0108 - val_loss: 0.0211 - val_acc: 0.0133\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 30s 493us/step - loss: 0.0279 - acc: 0.0107 - val_loss: 0.0211 - val_acc: 0.0117\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 28s 465us/step - loss: 0.0279 - acc: 0.0106 - val_loss: 0.0212 - val_acc: 0.0127\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 27s 451us/step - loss: 0.0279 - acc: 0.0107 - val_loss: 0.0212 - val_acc: 0.0153\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 27s 454us/step - loss: 0.0278 - acc: 0.0105 - val_loss: 0.0217 - val_acc: 0.0135\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3bcdae77f749>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                      \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      validation_data=(x_test, x_test))\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    history = sae_autoencoder.fit(x_train, x_train,\n",
    "                     epochs=50,\n",
    "                     batch_size=256,\n",
    "                     validation_data=(x_test, x_test))\n",
    "#     plt.plot(history.history['acc'])\n",
    "#     plt.plot(history.history['val_acc'])\n",
    "#     plt.title('model accuracy')\n",
    "#     plt.ylabel('accuracy')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.legend(['train', 'test'], loc='upper left')\n",
    "#     plt.show()\n",
    "    sae_autoencoder.save(f'sae_autoencoder_{i}.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 7s 113us/step\n"
     ]
    }
   ],
   "source": [
    "metrics = sae_autoencoder.evaluate(x_train, x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.023036169837911923\n",
      "acc: 0.012033333333333333\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sae_autoencoder.metrics_names)):\n",
    "    print(str(sae_autoencoder.metrics_names[i]) + \": \" + str(metrics[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sae_autoencoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e3cac728482c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msae_autoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# sae_autoencoder.predict(x_test[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m  \u001b[0;31m# how many digits we will display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sae_autoencoder' is not defined"
     ]
    }
   ],
   "source": [
    "predicted = sae_autoencoder.predict(x_test)\n",
    "import matplotlib.pyplot as plt\n",
    "# sae_autoencoder.predict(x_test[i])\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    print(x_test[i].shape)\n",
    "    plt.imshow(predicted[i].reshape(28,28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of DEC_autoencoder_test.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
